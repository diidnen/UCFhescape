# @package _global_

defaults:
  - launcher: default
  - model: default
  - training: default
  - paths: juelich
  - datamodule: lung_healthy
  - _self_
  - override paths/anatomy: lung_healthy

############ Job specific configurations ############

name: hescape_default_training

model:
  litmodule:
    img_enc_name: h0-mini
    img_finetune: true
    img_proj: mlp
    gene_enc_name: generic
    gene_finetune: false
    gene_proj: linear # mlp
    loss: CLIP
    temperature: 0.07

datamodule:
  seed: 24442
  batch_size: 64
  num_workers: 4
  pin_memory: true
  persistent_workers: true

training:
  lightning:
    trainer:
      max_steps: 20_000
      devices: ${devices_per_job}
      num_nodes: 1
      strategy: # "ddp"
        _target_: pytorch_lightning.strategies.DDPStrategy
        find_unused_parameters: true
        process_group_backend: "nccl"
      accelerator: "gpu"
  train: true
  test:
    false
    # profiler: simple

  logger:
    wandb:
      project: hescape_training

############ Hydra specific configurations ############
hydra:
  job:
    name: hescape_training
  sweeper:
    params:
      model.litmodule.img_enc_name: h0-mini
      model.litmodule.img_finetune: true  # Lora finetune
      model.litmodule.img_proj: mlp
      model.litmodule.gene_enc_name: generic # drvi is best performing
      model.litmodule.gene_finetune: false
      model.litmodule.gene_proj: linear
      model.litmodule.loss: CLIP
      model.litmodule.temperature: 0.07
      datamodule.seed: 24442
      datamodule.batch_size: 64
      datamodule.num_workers: 4
      training.train: true
      training.test: false
